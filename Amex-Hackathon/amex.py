# -*- coding: utf-8 -*-
"""Amex.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkqaWJ9KZnJS_itqL_KCQdJIqbQw-loU
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score,GridSearchCV,train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
from sklearn.preprocessing import Normalizer, StandardScaler, PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from datetime import datetime
from keras.callbacks import EarlyStopping
from keras.optimizers import SGD
import pandas as pd
import numpy as np
import os
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV,cross_val_score
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn import svm
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
# from catboost import CatBoostClassifier,Pool, cv
from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold,train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingClassifier,RandomForestClassifier
from sklearn.metrics import classification_report,confusion_matrix
#from hyperopt import fmin, tpe, hp, anneal, Trials
from sklearn.decomposition import PCA
# %matplotlib inline
from sklearn.pipeline import Pipeline
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'
from scipy.stats import boxcox
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import warnings
import calendar
warnings.filterwarnings('ignore')
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import TomekLinks
import glob
import tensorflow as tf
tf.set_random_seed(42)
import time
from sklearn.base import TransformerMixin

!pip install catboost

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My\ Drive/Colab\ Notebooks/AVChallenge/AmeXpert

# Import data set
df = glob.glob(r'/content/gdrive/My Drive/Colab Notebooks/AVChallenge/AmeXpert/train_AUpWtIz/*.csv')


df

# Assign values to dataset

df_campaign = pd.read_csv(df[0])
df_coupoun_item = pd.read_csv(df[1])
df_customer_demo = pd.read_csv(df[2])
df_customer_trans = pd.read_csv(df[3])
df_item_data = pd.read_csv(df[4])
df_train = pd.read_csv(df[5])
df_test = pd.read_csv(r'/content/gdrive/My Drive/Colab Notebooks/AVChallenge/AmeXpert/test.csv')

s = pd.read_csv(r'/content/gdrive/My Drive/Colab Notebooks/AVChallenge/AmeXpert/sample_submission.csv')

# Check the redemption status

df_train['redemption_status'].value_counts()

df_train.info()

df_campaign.head(4)

df_coupoun_item.head(3)

df_customer_demo.isna().sum()

df_customer_trans.head(3)

df_customer_trans.isna().sum()

df_item_data.head(3)

df_test.head(3)



# Architecture for generating the features_ Approach 1- 51% auc

# 1.Left join-> train & cust_demo 
# 2.Left join-> Resultant from step 1 & campaign_data
# 3.Inner join-> couponitem & item data
# 4.aggregate over 3 on coupon_id
# 5.Left Join Resultant from step 2 and 4 on coupon_id
# 6.Inner join-> cust_trans_data & item data
# 7.aggregate over 6 on customer id
# 8.left join-> Resultant from 2 & 7 on customer id
# Note: please aggregate in separate dataframes and then join to train. This will avoid confusion and record mismatch



# finding unique customer ids in train and test
df_train.customer_id.nunique(),df_test.customer_id.nunique()

# checking the customer id intersection with the customer demo and customer transaction
print (len(np.intersect1d(df_train.customer_id,df_customer_demo.customer_id)))
print (len(np.intersect1d(df_train.customer_id,df_customer_trans.customer_id)))
print (len(np.intersect1d(df_test.customer_id,df_customer_trans.customer_id)))
print (len(np.intersect1d(df_test.customer_id,df_customer_demo.customer_id)))


print (len(np.intersect1d(df_customer_trans.item_id,df_item_data.item_id)))
print (len(np.intersect1d(df_coupoun_item.item_id,df_item_data.item_id)))
print (len(np.intersect1d(df_coupoun_item.item_id,df_customer_trans.item_id)))

print (len(np.intersect1d(df_customer_demo.customer_id,df_customer_trans.customer_id)))


print (len(np.intersect1d(df_test.customer_id,df_train.customer_id)))

# Overlapp visible between train and test customer ids.
# Not all the cust ids are existing in the customer demographics.

# Before proceeding ahead with the architecture defined above. Let us treat some variables

df_customer_demo['no_of_children'] = df_customer_demo['no_of_children'].replace('3+', 3).astype(float)
df_customer_demo['family_size'] = df_customer_demo['family_size'].replace('5+', 5).astype(float)
df_customer_demo['marital_status'] = pd.Series(df_customer_demo['marital_status'].factorize()[0]).replace(-1,np.nan, inplace = True)
df_customer_demo['age_range'] = pd.Series(df_customer_demo['age_range'].factorize()[0])

df_customer_demo.head()

# Step 1: Left join-> train & cust_demo 

combined_train_cust_demo = df_train.merge(df_customer_demo, on = 'customer_id', how = 'left')
combined_train_cust_demo.info()

combined_train_cust_demo.isna().sum()

combined_train_cust_demo.head()

# Running on combined_train_cust_demo set for imputing missing values
class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, combined_train_cust_demo, y=None):

        self.fill = pd.Series([combined_train_cust_demo[c].value_counts().index[0]
           for c in combined_train_cust_demo],
            index=combined_train_cust_demo.columns)

        return self

    def transform(self, combined_train_cust_demo, y=None):
        return combined_train_cust_demo.fillna(self.fill,inplace = True)

# Calling function
DataFrameImputer().fit_transform(combined_train_cust_demo)

# step 2: Left join-> resultant from step 1 & campaign_data
combined_train_campaign_demo = combined_train_cust_demo.merge(df_campaign, on = 'campaign_id', how = 'left')
combined_train_campaign_demo.info()

combined_train_campaign_demo['customer_id'].nunique()

combined_train_campaign_demo['coupon_id'].nunique()

# Step 3 :Left join-> copoun item & item data
combined_coupoun_item = df_coupoun_item.merge(df_item_data, on = 'item_id', how='inner')
combined_coupoun_item.isna().sum()
combined_coupoun_item.info()

print (len(np.intersect1d(combined_train_campaign_demo.coupon_id,combined_coupoun_item.coupon_id)))

combined_coupoun_item['coupon_id'].nunique()

# Modifying the combined table above to generate consumable features

combined_coupoun_item.head()

combined_coupoun_item['brand_type'].unique()
combined_coupoun_item['category'].unique()

for k in combined_coupoun_item.columns:
    print(k,combined_coupoun_item[k].nunique())

combined_coupoun_item = pd.get_dummies(combined_coupoun_item,columns=['brand_type','category'])

combined_coupoun_item.info()

combined_coupoun_item.groupby(['coupon_id']).count()

combined_coupoun_item['item_id']  = combined_coupoun_item['item_id'].astype('category')
combined_coupoun_item['brand']  = combined_coupoun_item['brand'].astype('category')

cat_agg=['count','nunique']
agg_col={
    'item_id':cat_agg, 'brand':cat_agg}

for k in combined_coupoun_item.columns:
    if k.startswith('category') or k.startswith('brand_type_'):
        agg_col[k] = ['sum','mean']
    else:
        agg_col

combined_coupoun_item1 = combined_coupoun_item.groupby(['coupon_id']).agg(agg_col)

combined_coupoun_item1.info()

combined_coupoun_item1.columns=['J_' + '_'.join(col).strip() for col in combined_coupoun_item1.columns.values]
combined_coupoun_item1.reset_index(inplace=True)
combined_coupoun_item1.head()
combined_coupoun_item1.shape

sns.scatterplot(combined_coupoun_item1['J_item_id_count'],combined_coupoun_item1['J_item_id_nunique'])

combined_coupoun_item1.drop(columns= 'J_item_id_count', inplace= True)

combined_coupoun_item1.info()

# combined_coupoun_item1[combined_coupoun_item1.duplicated()]

# Step 3.1--aggregate over step 3 resultant on coupon_id

# combined_coupoun_item.groupby(['coupon_id'], as_index=False).item_id.agg('nunique').rename(columns={'count':'item_counts'}).reset_index().head()

# combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'item_counts'}).head()
# print ('\n')

# combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['item_id']].mean().rename(columns={'item_id':'item_counts'}).head()
# # print ('\n')
# combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand']].count().rename(columns={'brand':'brand_counts'}).head()
# print ('\n')
# combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'brand_type_counts'}).head()
# print ('\n')
# combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['category']].count().rename(columns={'category_type':'category_type_counts'}).head()

combined_train_campaign_demo.shape
combined_coupoun_item1.shape

# Step 4 : inner join : Resultant from step 2 and Step 3 on coupon id.

combined_train_campaign_demo_item1_data = combined_train_campaign_demo.merge(combined_coupoun_item1, on = 'coupon_id', how = 'inner')


# combined_train_campaign_demo_item2_data = combined_train_campaign_demo.merge(combined_coupoun_item1, on = 'coupon_id', how = 'left')

combined_train_campaign_demo_item1_data.shape

# Step 4 : Left join : Resultant from step 2 and Step 3 operations on multiple types like brand, brand type.

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'item_counts'}), how='inner', on='coupon_id')
# # combined_train_campaign_demo['item_counts'].fillna(0, inplace=True)

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand']].count().rename(columns={'brand':'brand_counts'}), how='inner', on='coupon_id')
# # combined_train_campaign_demo['brand_counts'].fillna(0, inplace=True)

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'brand_type_counts'}),how='inner', on='coupon_id')
# # combined_train_campaign_demo['brand_type_counts'].fillna(0, inplace=True)


# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['category']].count().rename(columns={'category':'category_type_counts'}),how='inner', on='coupon_id')
# # combined_train_campaign_demo['category_type_counts'].fillna(0, inplace=True)

combined_train_campaign_demo_item1_data[combined_train_campaign_demo_item1_data.duplicated()]
combined_train_campaign_demo_item1_data.isna().sum()

# Step 5: Inner join on cust trans data and itemdata on item id
combined_cust_item_data = df_customer_trans.merge(df_item_data, on = 'item_id', how = 'inner')
combined_cust_item_data.info()

combined_cust_item_data.isna().sum()

test = combined_cust_item_data.merge(df_customer_demo, on = 'customer_id', how = 'left')
test.info()
test.isna().sum()

test['customer_id'].nunique()

combined_cust_item_data.head()

combined_cust_item_data.info()

# combined_cust_item_data[combined_cust_item_data.duplicated()]
# combined_cust_item_data.duplicated()

combined_cust_item_data.drop_duplicates(inplace= True)

# Running on combined_train_cust_demo set for imputing missing values
class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, combined_cust_item_data, y=None):

        self.fill = pd.Series([combined_cust_item_data[c].value_counts().index[0]
           for c in combined_cust_item_data],
            index=combined_cust_item_data.columns)

        return self

    def transform(self, combined_cust_item_data, y=None):
        return combined_cust_item_data.fillna(self.fill,inplace = True)

DataFrameImputer().fit_transform(combined_cust_item_data)

combined_cust_item_data.info()

# Aggregaiton and Feature generation on combined columns:

for k in combined_cust_item_data.columns:
    print(k,combined_cust_item_data[k].nunique())

# combined_cust_item_data.drop(columns=['date'],inplace=True)

combined_cust_item_data = pd.get_dummies(combined_cust_item_data,columns=['brand_type','category','age_range','marital_status'], prefix = 'Trans', drop_first=True)

combined_cust_item_data = pd.get_dummies(combined_cust_item_data,columns=['family_size','no_of_children'], drop_first=True)

combined_cust_item_data.info()

combined_cust_item_data['date'] = pd.to_datetime(combined_cust_item_data['date'],format= '%Y-%m-%d')

combined_cust_item_data.sort_values(['customer_id','date'],ascending=True,inplace=True)
combined_cust_item_data['cumcount_1'] = combined_cust_item_data.groupby('customer_id')['quantity'].cumcount()+1

combined_cust_item_data.sort_values(['customer_id','item_id','date'],ascending=True,inplace=True)
combined_cust_item_data['cumcount_2'] = combined_cust_item_data.groupby(['customer_id','item_id'])['quantity'].cumcount()+1

combined_cust_item_data.sort_values(['customer_id','quantity','item_id','date'],ascending=True,inplace=True)
combined_cust_item_data['cumcount_3'] = combined_cust_item_data.groupby(['customer_id','item_id','quantity'])['quantity'].cumcount()+1

combined_cust_item_data['item_id']  = combined_cust_item_data['item_id'].astype('category')
combined_cust_item_data['brand']  = combined_cust_item_data['brand'].astype('category')
combined_cust_item_data['income_bracket'] = combined_cust_item_data['income_bracket'].astype('category')
combined_cust_item_data['rented'] = combined_cust_item_data['rented'].astype('category')

combined_cust_item_data.drop(columns=['date'],inplace = True)

combined_cust_item_data.head()

combined_cust_item_data['customer_id'].nunique()

cat_agg=['count','nunique']
num_agg=['min','mean','max','sum']
agg_col={
    'item_id':cat_agg, 'brand':cat_agg, 'quantity':cat_agg,
       'selling_price':['count','nunique','mean'], 'other_discount':['count','nunique','mean'],'coupon_discount':['count','nunique','mean']
}

for k in combined_cust_item_data.columns:
    if k.startswith('Trans_'):
        agg_col[k]=['sum','mean']
    elif k.startswith('family_size_') or k.startswith('no_of_children'):
        agg_col[k]=['sum','mean']
    elif k.startswith('cumcount'):
        agg_col[k]=num_agg
agg_col

combined_cust_item_data1= combined_cust_item_data.groupby('customer_id').agg(agg_col)

combined_cust_item_data1.head()

combined_cust_item_data1.columns=['T_' + '_'.join(col).strip() for col in combined_cust_item_data1.columns.values]
combined_cust_item_data1.reset_index(inplace=True)
combined_cust_item_data1.head()

combined_cust_item_data1.shape

# # Step 6: Aggregate resultant from step 5 on customer id. This is at customer level information.

# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'customerid_item_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand']].count().rename(columns={'brand':'customerid_brand_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'customerid_brand_type_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['category']].count().rename(columns={'category_type':'customerid_category_type_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['quantity']].count().rename(columns={'quantity':'customerid_quantity_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['other_discount']].count().rename(columns={'other_discount':'customerid_other_discount_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['coupon_discount']].count().rename(columns={'coupon_discount':'customerid_coupon_discount_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['selling_price']].count().rename(columns={'selling_price':'customerid_sellingprice_counts'}).head()
# print ('\n')
# combined_cust_item_data.groupby(['customer_id'], as_index=False)[['date']].count().rename(columns={'date':'customerid_date_counts'}).head()

combined_cust_item_data1.info()

combined_train_campaign_demo_item1_data.shape

combined_train_campaign_demo_item1_data['customer_id'].nunique()

combined_cust_item_data1['customer_id'].nunique()

print(len(np.intersect1d(combined_train_campaign_demo_item1_data.customer_id,combined_cust_item_data1.customer_id)))

# combined_Tr_Trinne_data = combined_train_campaign_demo_item1_data.merge(combined_cust_item_data1, on = 'customer_id', how = 'inner')
combined_EA_data = combined_train_campaign_demo_item1_data.merge(combined_cust_item_data1, on = 'customer_id', how = 'inner')

# combined_Tr_Trinne_data.shape
combined_EA_data.shape

combined_EA_data['customer_id'].nunique()

# # Step 7: Left Join--> combined_train_campaign_demo and step 6



# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'customerid_item_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo  = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand']].count().rename(columns={'brand':'customerid_brand_counts'}), on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'customerid_brand_type_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['category']].count().rename(columns={'category_type':'customerid_category_type_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['quantity']].count().rename(columns={'quantity':'customerid_quantity_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['other_discount']].count().rename(columns={'other_discount':'customerid_other_discount_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['coupon_discount']].count().rename(columns={'coupon_discount':'customerid_coupon_discount_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['selling_price']].count().rename(columns={'selling_price':'customerid_sellingprice_counts'}),on = 'customer_id', how = 'left')

# combined_train_campaign_demo = combined_train_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['date']].count().rename(columns={'date':'customerid_date_counts'}),on = 'customer_id', how = 'left')

combined_EA_data.info()

"""# Create Test Data set"""

# Step 11: Left join-> test & cust_demo 

combined_test_cust_demo = df_test.merge(df_customer_demo, on = 'customer_id', how = 'left')
combined_test_cust_demo.info()

# step 12: Left join-> resultant from step 1 & campaign_data

combined_test_campaign_demo = combined_test_cust_demo.merge(df_campaign, on = 'campaign_id', how = 'left')
combined_test_campaign_demo.info()

# Running on combined_test_cust_demo set for imputing missing values
class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, combined_test_cust_demo, y=None):

        self.fill = pd.Series([combined_test_cust_demo[c].value_counts().index[0]
           for c in combined_test_cust_demo],
            index=combined_test_cust_demo.columns)

        return self

    def transform(self, combined_test_cust_demo, y=None):
        return combined_test_cust_demo.fillna(self.fill,inplace = True)

DataFrameImputer().fit_transform(combined_test_cust_demo)
DataFrameImputer().fit_transform(combined_test_campaign_demo)

combined_test_cust_demo.isna().sum()
combined_test_campaign_demo.isna().sum()

# # Step 13 : Inner join : Resultant from step 12 and Step 3 operations on multiple types like brand, brand type.

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'item_counts'}), how='inner', on='coupon_id')
# combined_test_campaign_demo['item_counts'].fillna(0, inplace=True)

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand']].count().rename(columns={'brand':'brand_counts'}), how='inner', on='coupon_id')
# combined_test_campaign_demo['brand_counts'].fillna(0, inplace=True)

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'brand_type_counts'}),how='inner', on='coupon_id')
# combined_test_campaign_demo['brand_type_counts'].fillna(0, inplace=True)


# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_coupoun_item.groupby(['coupon_id'], as_index=False)[['category']].count().rename(columns={'category':'category_type_counts'}),how='inner', on='coupon_id')
# combined_test_campaign_demo['category_type_counts'].fillna(0, inplace=True)

combined_test_campaign_demo_item1_data = combined_test_campaign_demo.merge(combined_coupoun_item1, on = 'coupon_id', how = 'inner')

combined_test_campaign_demo_item1_data.shape

combined_test_campaign_demo_item1_data.info()

combined_test_campaign_demo_item1_data.isna().sum()

# Step 14: Left Join--> combined_test_campaign_demo and resultant from step 6


combined_EA_Test_data = combined_test_campaign_demo_item1_data.merge(combined_cust_item_data1, on = 'customer_id', how = 'left')
combined_EA_Test_data.shape



# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['item_id']].count().rename(columns={'item_id':'customerid_item_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo  = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand']].count().rename(columns={'brand':'customerid_brand_counts'}), on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['brand_type']].count().rename(columns={'brand_type':'customerid_brand_type_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['category']].count().rename(columns={'category_type':'customerid_category_type_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['quantity']].count().rename(columns={'quantity':'customerid_quantity_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['other_discount']].count().rename(columns={'other_discount':'customerid_other_discount_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['coupon_discount']].count().rename(columns={'coupon_discount':'customerid_coupon_discount_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['selling_price']].count().rename(columns={'selling_price':'customerid_sellingprice_counts'}),on = 'customer_id', how = 'left')

# combined_test_campaign_demo = combined_test_campaign_demo.merge(combined_cust_item_data.groupby(['customer_id'], as_index=False)[['date']].count().rename(columns={'date':'customerid_date_counts'}),on = 'customer_id', how = 'left')

combined_EA_Test_data.info()

combined_test_campaign_demo.isna().sum()

combined_EA_data.shape, combined_EA_Test_data.shape

"""## Feature Generation Starts"""

# Check for duplicated rows if any after merges
combined_EA_data[combined_EA_data.duplicated()]

combined_EA_Test_data[combined_EA_Test_data.duplicated()]

combined_EA_data['start_date'] = pd.to_datetime(combined_EA_data['start_date'], format='%d/%m/%y')
combined_EA_data['end_date'] = pd.to_datetime(combined_EA_data['end_date'], format='%d/%m/%y')

combined_EA_Test_data['start_date'] = pd.to_datetime(combined_EA_Test_data['start_date'], format='%d/%m/%y')
combined_EA_Test_data['end_date'] = pd.to_datetime(combined_EA_Test_data['end_date'], format='%d/%m/%y')

combined_EA_data.info()

combined_EA_data['age_range'].unique()

combined_EA_Test_data['age_range'].value_counts()

combined_EA_data = pd.get_dummies(columns= ['campaign_type','age_range','family_size','marital_status','no_of_children'], data = combined_EA_data)

combined_EA_Test_data = pd.get_dummies(columns= ['campaign_type','age_range','family_size','marital_status','no_of_children'], data = combined_EA_Test_data)

combined_EA_data['start_date_year'] = combined_EA_data['start_date'].dt.year 
combined_EA_data['start_date_month'] = combined_EA_data['start_date'].dt.month 
combined_EA_data['start_date_day'] = combined_EA_data['start_date'].dt.day

combined_EA_data['end_date_year'] = combined_EA_data['end_date'].dt.year 
combined_EA_data['end_date_month'] = combined_EA_data['end_date'].dt.month 
combined_EA_data['end_date_day'] = combined_EA_data['end_date'].dt.day

combined_EA_Test_data['start_date_year'] = combined_EA_Test_data['start_date'].dt.year 
combined_EA_Test_data['start_date_month'] = combined_EA_Test_data['start_date'].dt.month 
combined_EA_Test_data['start_date_day'] = combined_EA_Test_data['start_date'].dt.day
combined_EA_Test_data['end_date_year'] = combined_EA_Test_data['end_date'].dt.year 
combined_EA_Test_data['end_date_month'] = combined_EA_Test_data['end_date'].dt.month 
combined_EA_Test_data['end_date_day'] = combined_EA_Test_data['end_date'].dt.day

combined_EA_data.drop(columns=['start_date_year','end_date_year','start_date','end_date'], inplace= True)
combined_EA_Test_data.drop(columns=['start_date_year','end_date_year','start_date','end_date'], inplace= True)

combined_EA_Test_data.info()

X,y=combined_EA_data.drop(['id','redemption_status'],axis=1),combined_EA_data['redemption_status']
Xtest=combined_EA_Test_data.drop(['id'],axis=1)

print(X.shape,Xtest.shape)
X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state = 1994,stratify=y)

seed = 2

pipeline_XGB = Pipeline([
                     ("sc", StandardScaler()),
             ("XGBR", XGBClassifier(seed=seed))
                 ])

param_grid_XGB = {
'XGBR__max_depth':range(3,10,2),
'XGBR__min_child_weight':range(1,6,2),
'XGBR__gamma':[i/10.0 for i in range(0,5)],
'XGBR__subsample':[i/10.0 for i in range(6,10)],
'XGBR__colsample_bytree':[i/10.0 for i in range(6,10)],
'XGBR__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],
'XGBR__learning_rate' : [0.1,0.001,0.3] ,
'XGBR__n_estimators': [int(x) for x in np.linspace(start = 10, stop = 200, num = 10)],
'XGBR__nthread' : [2,3,5] , 
'XGBR__scale_pos_weight' : 1,
'XGBR__scale_pos_weight': [1,2]
    }

RandomCV = RandomizedSearchCV(pipeline_XGB,param_grid_XGB,cv=3)
RandomCV.fit(X_train,y_train)
preds=RandomCV.predict(X_test)
print ('\n')
# print ("Best Parameter for RandomCV is {}".format(RandomCV.best_estimator_))
# print ('\n')
print ("The Score for XGB is {}".format(roc_auc_score(y_val,preds)))

m = pipeline_XGB.fit(X_train,y_train)
preds=m.predict(X_test)
print("score: ",roc_auc_score(y_test,preds))
score.append(roc_auc_score(y_test,preds))

# combining the entire train and test together for feature gen operations

df = pd.concat([combined_EA_data,combined_EA_Test_data], axis = 0)

df.info()

# plt.figure(figsize = (15,15))
# sns.heatmap(df.corr(),annot = True)
# plt.show()

df['age_range'].value_counts()

df['family_size'].value_counts()

df['marital_status'].value_counts()

df['no_of_children'].value_counts()

df = pd.get_dummies(columns= ['campaign_type','age_range','family_size','marital_status','no_of_children'], data = df)

df.info()

df['start_date'] = pd.to_datetime(df['start_date'], format='%d/%m/%y')
df['end_date'] = pd.to_datetime(df['end_date'], format='%d/%m/%y')

df['start_date_year'] = df['start_date'].dt.year 
df['start_date_month'] = df['start_date'].dt.month 
df['start_date_day'] = df['start_date'].dt.day

df['end_date_year'] = df['end_date'].dt.year 
df['end_date_month'] = df['end_date'].dt.month 
df['end_date_day'] = df['end_date'].dt.day

df['start_date_month'].unique()

df['end_date_month'].unique()

df['end_date_day'].unique()

df['start_date_year'].unique()

df['end_date_year'].unique()

df.drop(columns=['start_date_year','end_date_year','start_date','end_date'], inplace= True)

# df.drop(columns=[], inplace= True)

df.shape

# df[df['redemption_status'].isnull()].count()

df_train_ds=df[df['redemption_status'].isnull()==False].copy()
df_test=df[df['redemption_status'].isnull()==True].copy()

print(df_train_ds.shape,df_test.shape)

X,y=df_train_ds.drop(['id','redemption_status'],axis=1),df_train_ds['redemption_status']
Xtest=df_test.drop(['id','redemption_status'],axis=1)

print(X.shape,Xtest.shape)
X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state = 1994,stratify=y)

df_train['redemption_status'].value_counts()/df_train.shape[0]

X.isnull().sum()



score=[]
y_pred_tot=[]
from sklearn.model_selection import KFold,StratifiedKFold
fold=StratifiedKFold(n_splits=10,shuffle=True,random_state=1994)
i=1
for train_index, test_index in fold.split(X,y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y[train_index], y[test_index]
    m=LGBMClassifier(n_estimators=3000,random_state=1994,learning_rate=0.001,colsample_bytree=0.2,objective='binary',scale_pos_weight=1)
    m.fit(X_train,y_train,eval_set=[(X_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=200)
    preds=m.predict(X_test)
    print("score: ",roc_auc_score(y_test,preds))
    score.append(roc_auc_score(y_test,preds))
    p = m.predict(Xtest)
    i=i+1
    y_pred_tot.append(p)

y_pred_tot[0]

s['redemption_status']=np.mean(y_pred_tot,0)
s.head()

s['redemption_status'].unique()

s.to_csv('C:\E Drive\Mixed Materials\AI-ML\Python Code and Practice\Analytics Vidhya\AmeXpert\Sample_submission_1.csv', index = False)

s.shape

y_pre

################################################ overall accuracy in Leader board was 85%.